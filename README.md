# AgentX - RAG-First Conversational AI Agent

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A production-ready **Retrieval-Augmented Generation (RAG)** system with intelligent agent, knowledge base, and web grounding.

**Version:** 0.0.1 | **Author:** Mahir Baig

---

## Overview

AgentX is a conversational AI agent featuring:

- **RAG-First Workflow** - Internal knowledge base before web search
- **Multi-Modal I/O** - Text, speech-to-text, text-to-speech
- **Conversation Memory** - Thread-based context across turns
- **FastAPI + Streamlit** - REST API and interactive web UI
- **Azure Integration** - OpenAI, Blob Storage, Cognitive Services

---

## Architecture

### Standard RAG Pattern: Retrieve ‚Üí Augment ‚Üí Generate

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   INTERFACE LAYER                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Streamlit UI       ‚îÇ        FastAPI REST API              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            ORCHESTRATION LAYER                              ‚îÇ
‚îÇ         LangGraph Agent + Conversation Memory               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ               ‚îÇ               ‚îÇ
    RETRIEVE        GENERATE        GROUNDING
    (RAG Tool)      (LLM)          (Fallback)
        ‚îÇ               ‚îÇ               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   DATA LAYER                                ‚îÇ
‚îÇ  ChromaDB Vector Store  ‚îÇ  Azure Blob Storage               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  SERVICE LAYER                               ‚îÇ
‚îÇ  Azure OpenAI (Embeddings + LLM)  ‚îÇ  Perplexity (Web Search)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Execution Flows

### üì§ Document Ingestion (Offline - Once per document)

```
Upload PDF/TXT
    ‚Üì
Extract Text ‚Üí Chunk (1000 tokens) ‚Üí Embed (Azure OpenAI)
    ‚Üì
Store in ChromaDB + Azure Blob
    ‚Üì
‚úÖ Ready for Search
```

### üí¨ Query Processing (Online - Every query)

```
User Question
    ‚Üì
Embed Query ‚Üí Search ChromaDB (Top-3 similar chunks)
    ‚Üì
    ‚îú‚îÄ Found? ‚Üí Generate answer with LLM + Citations ‚Üí Done ‚úÖ
    ‚îî‚îÄ Not Found? ‚Üí Web Search (Perplexity) ‚Üí Done ‚úÖ
```

**Key Rule:** RAG always runs first. Grounding only if RAG finds nothing.

---

## Quick Start

### Installation

```bash
# Clone and setup
git clone https://github.com/Mahir-Baig/Agentx.git
cd AgentX
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
pip install -e .

# Configure environment
cp .env.example .env
# Edit .env with your API keys
```

### Configuration (.env)

```bash
# Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_DEPLOYMENT=gpt-4-mini
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small

# Azure Storage
AZURE_STORAGE_CONNECTION_STRING=your-connection-string
AZURE_STORAGE_CONTAINER_NAME=documents

# Perplexity (Web Grounding)
PERPLEXITY_API_KEY=your-perplexity-key

# LangSmith (Optional - for tracing)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-key
```

### Run

```bash
# Option 1: Streamlit UI
streamlit run app.py

# Option 2: FastAPI Backend
uvicorn src.api:app --reload

# Docker
docker build -t agentx-api .
docker run -p 8000:8000 --env-file .env agentx-api
```

---

## Project Structure

```
AgentX/
‚îú‚îÄ‚îÄ app.py                      # Streamlit UI
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api.py                  # FastAPI REST API
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.py            # LangGraph RAG Agent
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.py              # RAG retrieval (runs first)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ grounding.py        # Web search (fallback)
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py      # LLM provider
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_database.py  # ChromaDB manager
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ azure_blob_service.py
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.py        # Text embeddings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunking.py         # Text chunking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ extractor.py        # PDF/TXT extraction
‚îÇ   ‚îî‚îÄ‚îÄ pipelines/
‚îÇ       ‚îî‚îÄ‚îÄ document_pipeline.py # Document processing
‚îú‚îÄ‚îÄ data/chromadb/              # Vector database
‚îî‚îÄ‚îÄ logs/                       # Application logs
```

---

## API Reference

### POST `/query`

```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is AgentX?",
    "thread_id": "optional-uuid"
  }'
```

**Response:**
```json
{
  "success": true,
  "response": "Based on your documents, AgentX is...",
  "thread_id": "uuid",
  "sources": [{"document": "doc.pdf", "page": 5}]
}
```

### POST `/upload`

```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@document.pdf"
```

### GET `/health`

```bash
curl http://localhost:8000/health
```

---

## Usage Example

```python
import requests

# Query with conversation memory
thread_id = "user-123"
response = requests.post(
    "http://localhost:8000/query",
    json={
        "query": "What are the key features?",
        "thread_id": thread_id
    }
)

print(response.json()["response"])
```

---

## How RAG Works

1. **Upload Document** ‚Üí Extract text ‚Üí Chunk ‚Üí Embed ‚Üí Store in ChromaDB
2. **User Asks Question** ‚Üí Embed query ‚Üí Search similar chunks
3. **Found Docs?**
   - **Yes** ‚Üí LLM generates answer using docs as context
   - **No** ‚Üí Search web via Perplexity API
4. **Return Answer** with citations/sources

**Example:**
```
Query: "What is AgentX?"
‚Üí Search ChromaDB (finds 3 chunks, similarity > 0.7)
‚Üí LLM: "Based on your documents, AgentX is a RAG system..."
‚Üí Citation: [doc.pdf, page 1]
```

---

## Key Features

- **RAG-First**: Always checks internal knowledge before web
- **Sequential Execution**: RAG ‚Üí Grounding (never parallel)
- **Conversation Memory**: Maintains context via thread IDs
- **Semantic Search**: Vector embeddings (not just keywords)
- **Multi-Modal**: Text input + voice (STT/TTS)
- **Production Ready**: FastAPI, Docker, logging, tracing

---

## System Requirements

- Python 3.10+
- 4GB RAM (8GB recommended)
- Azure Account (OpenAI, Storage, Cognitive Services)
- Internet connection

---

## Configuration

### Chunking (`src/components/chunking.py`)
```python
CHUNK_SIZE = 1000          # Tokens per chunk
CHUNK_OVERLAP = 200        # Overlap between chunks
```

### Retrieval (`src/tools/rag.py`)
```python
TOP_K = 3                  # Number of chunks to retrieve
SIMILARITY_THRESHOLD = 0.7 # Minimum similarity (0-1)
```

### LLM (`src/services/llm_service.py`)
```python
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 500
```

---

## Troubleshooting

**Azure credentials not found:**
```bash
cat .env | grep AZURE_OPENAI
```

**ChromaDB error:**
```bash
mkdir -p data/chromadb
rm -rf data/chromadb/  # If corrupted
```

**Port already in use:**
```bash
streamlit run app.py --server.port 8502
```

**Import errors:**
```bash
pip uninstall agentx -y
pip install -e .
```

---

## Testing

```bash
# Integration tests
python tests/test_complete_rag.py

# API test
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "test"}'

# UI test
streamlit run app.py
```

---

## Deployment

### Docker
```bash
docker build -t agentx-api .
docker run -p 8000:8000 --env-file .env agentx-api
```

### Azure Container Apps
```bash
az acr build --registry <your-acr> --image agentx:v1 .
az containerapp create \
  --name agentx-api \
  --resource-group <your-rg> \
  --image <your-acr>.azurecr.io/agentx:v1
```

---

## Dependencies

- **LangChain** - LLM orchestration
- **LangGraph** - Agent workflows
- **FastAPI** - REST API
- **Streamlit** - Web UI
- **ChromaDB** - Vector database
- **Azure OpenAI** - Embeddings + LLM
- **Perplexity API** - Web search

See `requirements.txt` for full list.

---

## FAQ

**Q: Can I use OpenAI API directly?**  
A: Yes, modify `llm_service.py` to use OpenAI instead of Azure OpenAI.

**Q: How much does it cost?**  
A: ~$17-30 per 1000 queries (GPT-4-mini + embeddings + storage).

**Q: Can I run offline?**  
A: No, requires Azure services. Use Ollama + local embeddings for offline.

**Q: How many documents?**  
A: ChromaDB scales to millions. Local: 10K-100K, Cloud: 100K-1M+.

---

## Resources

- **LangChain:** https://python.langchain.com/
- **ChromaDB:** https://docs.trychroma.com/
- **Azure OpenAI:** https://azure.microsoft.com/en-us/products/ai-services/openai-service/
- **GitHub Issues:** https://github.com/Mahir-Baig/Agentx/issues

---

## License

MIT License - See [LICENSE](LICENSE) file for details.

---

## Author

**Mahir Baig**  
GitHub: [@Mahir-Baig](https://github.com/Mahir-Baig)  
Email: mahirbaig2@gmail.com  
Project: [AgentX](https://github.com/Mahir-Baig/Agentx)

---

**Last Updated:** December 30, 2025  
**Version:** 0.0.1

---

**Built with ‚ù§Ô∏è by Mahir Baig**