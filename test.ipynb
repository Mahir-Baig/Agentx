{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb016f4d",
   "metadata": {},
   "source": [
    "### Read the files using langchain and store all the text in a single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561d62ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\user\\anaconda3\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\user\\anaconda3\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pypdf in c:\\users\\user\\anaconda3\\lib\\site-packages (6.4.0)\n",
      "Requirement already satisfied: python-docx in c:\\users\\user\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (1.1.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.53)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdf2image) (10.4.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-docx) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (24.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.12)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.12.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (2.1)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community pdf2image pypdf python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f2349d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sonata_POC'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8897a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sonata_POC\\\\docs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path = os.path.join(os.getcwd(), 'docs')\n",
    "files_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd2a1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25939_Srikrupa_Cv-1.docx',\n",
       " '26107_Mahir Baig2.pdf',\n",
       " 'design-document.md',\n",
       " 'test-cases.md',\n",
       " 'test-report.md']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24054cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25939_Srikrupa_Cv-1.docx\n",
      "26107_Mahir Baig2.pdf\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(files_path):\n",
    "    if i.endswith('.pdf'):\n",
    "        print(i)\n",
    "    elif i.endswith('.docx'):\n",
    "        print(i)\n",
    "    elif i.endswith('.txt'):\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf0e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 3 documents\n",
      "✓ Total text length: 14124 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "# Function to load DOCX files\n",
    "def load_docx_files(directory):\n",
    "    \"\"\"Load all DOCX files from directory\"\"\"\n",
    "    documents = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.docx'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                doc = Document(file_path)\n",
    "                text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "                documents.append({\n",
    "                    'content': text,\n",
    "                    'source': file_path\n",
    "                })\n",
    "    return documents\n",
    "\n",
    "# Load all documents from the directory\n",
    "pdf_loader = DirectoryLoader(files_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "txt_loader = DirectoryLoader(files_path, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "documents = []\n",
    "documents.extend(pdf_loader.load())\n",
    "documents.extend(txt_loader.load())\n",
    "documents.extend(load_docx_files(files_path))\n",
    "\n",
    "# Combine all text into a single variable\n",
    "all_text = \"\\n\\n\".join([\n",
    "    doc.page_content if hasattr(doc, 'page_content') else doc['content'] \n",
    "    for doc in documents\n",
    "])\n",
    "\n",
    "print(f\"✓ Loaded {len(documents)} documents\")\n",
    "print(f\"✓ Total text length: {len(all_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccab9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9ef547",
   "metadata": {},
   "source": [
    "### Convert Single variables to vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193f81b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5222eb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedding created\n",
      "✓ Embedding shape: (384,)\n",
      "✓ Vector dimensions: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert the text into vectors\n",
    "text_embedding = model.encode(all_text)\n",
    "\n",
    "print(f\"✓ Embedding created\")\n",
    "print(f\"✓ Embedding shape: {text_embedding.shape}\")\n",
    "print(f\"✓ Vector dimensions: {len(text_embedding[0]) if isinstance(text_embedding, list) else text_embedding.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49691d",
   "metadata": {},
   "source": [
    "### Store in Chroma db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf623503",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93929922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stored 10 document chunks in Chroma DB\n",
      "✓ Collection name: document_embeddings\n",
      "✓ Collection count: 10\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import hashlib\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Delete existing collection if it exists\n",
    "try:\n",
    "    client.delete_collection(name=\"document_embeddings\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create fresh collection\n",
    "collection = client.get_or_create_collection(name=\"document_embeddings\")\n",
    "\n",
    "# Prepare document chunks and metadata\n",
    "doc_contents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "# Split documents into chunks (512 tokens with 20 token overlap for RAG)\n",
    "chunk_size = 2000  # approximate characters\n",
    "overlap = 400\n",
    "\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    # Get document content\n",
    "    content = doc.page_content if hasattr(doc, 'page_content') else doc['content']\n",
    "    source = doc.metadata.get('source') if hasattr(doc, 'metadata') else doc.get('source', f'doc_{doc_idx}')\n",
    "    \n",
    "    # Create chunks\n",
    "    for chunk_idx in range(0, len(content), chunk_size - overlap):\n",
    "        chunk = content[chunk_idx:chunk_idx + chunk_size]\n",
    "        \n",
    "        if len(chunk.strip()) > 50:  # Skip very small chunks\n",
    "            chunk_id = f\"{doc_idx}_chunk_{chunk_idx}\"\n",
    "            doc_contents.append(chunk)\n",
    "            ids.append(chunk_id)\n",
    "            metadatas.append({\n",
    "                'source': str(source),\n",
    "                'chunk_idx': chunk_idx,\n",
    "                'doc_idx': doc_idx\n",
    "            })\n",
    "\n",
    "# Add documents with their embeddings to Chroma\n",
    "if doc_contents:\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=doc_contents,\n",
    "        metadatas=metadatas,\n",
    "        embeddings=[model.encode(doc).tolist() for doc in doc_contents]\n",
    "    )\n",
    "    print(f\"✓ Stored {len(ids)} document chunks in Chroma DB\")\n",
    "    print(f\"✓ Collection name: {collection.name}\")\n",
    "    print(f\"✓ Collection count: {collection.count()}\")\n",
    "else:\n",
    "    print(\"✗ No documents to store in Chroma DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfdb650",
   "metadata": {},
   "source": [
    "### search-cosine search or hybrid serach ---- chuncks retrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac89f67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9efc2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the experience of mahir?\n",
      "\n",
      "Found 5 relevant chunks:\n",
      "\n",
      "--- Result 1 ---\n",
      "Source: d:\\Sonata_POC\\docs\\26107_Mahir Baig2.pdf\n",
      "Similarity Score: -0.6325\n",
      "Content: Summary \n",
      " \n",
      "                                                       Mahir Baig \n",
      "                                                           GenAI Engineer  \n",
      " \n",
      "Bangalore 9066340827 mahirbaig2@gmail.com \n",
      "I am a highly Accomplished Data Science professional with 4+ years of experience in predictive modeli...\n",
      "\n",
      "--- Result 2 ---\n",
      "Source: d:\\Sonata_POC\\docs\\25939_Srikrupa_Cv-1.docx\n",
      "Similarity Score: -0.6855\n",
      "Content: Sri Krupa R\n",
      "\tGenAI Engineer\t\n",
      "\n",
      "Bangalore\t7676200408\tsrikrupa738@gmail.com\n",
      "Highly accomplished Data Science professional with over 4+ years of experience in solving complex challenges across diverse sectors. Strong expertise in predictive modeling, advanced analytics, and machine learning, with 2+ yea...\n",
      "\n",
      "--- Result 3 ---\n",
      "Source: d:\\Sonata_POC\\docs\\26107_Mahir Baig2.pdf\n",
      "Similarity Score: -0.7499\n",
      "Content: ews for solar design and installation \n",
      "services. The project involves translating reviews from multiple languages to English and \n",
      "then performing sentiment analysis to gauge customer satisfaction.  \n",
      " \n",
      "Key Highlights: \n",
      " \n",
      "• Data Collection: Gather a dataset  of customer  reviews in various languages \n",
      "...\n",
      "\n",
      "--- Result 4 ---\n",
      "Source: d:\\Sonata_POC\\docs\\25939_Srikrupa_Cv-1.docx\n",
      "Similarity Score: -0.7730\n",
      "Content:  FaceNet's deep learning architecture. The goal was to create an innovative solution that combines state-of-the-art facial recognition technology with Azure's robust infrastructure to deliver secure and seamless user authentication.\n",
      "\n",
      "Key Highlights:\n",
      "\n",
      "Precise Facial Embeddings: Utilizing FaceNet's de...\n",
      "\n",
      "--- Result 5 ---\n",
      "Source: d:\\Sonata_POC\\docs\\26107_Mahir Baig2.pdf\n",
      "Similarity Score: -0.9120\n",
      "Content: ensure consistency, scalability, and \n",
      "effective knowledge sharing within the team.   \n",
      "• Supported the implementation and troubleshooti ng of AI \n",
      "services such as Speech, Translator, and Document \n",
      "Intelligence, ensuring high availability and optimal \n",
      "performance. \n",
      "• Collaborated with engineering and ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to search and retrieve chunks from ChromaDB\n",
    "def search_documents(query, n_results=5):\n",
    "    \"\"\"\n",
    "    Search for relevant document chunks using cosine similarity\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        n_results: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing search results with documents and metadata\n",
    "    \"\"\"\n",
    "    # Encode the query using the same model\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "    \n",
    "    # Search in the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the experience of mahir?\"\n",
    "search_results = search_documents(query, n_results=5)\n",
    "\n",
    "# Display results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Found {len(search_results['documents'][0])} relevant chunks:\\n\")\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    search_results['documents'][0], \n",
    "    search_results['metadatas'][0],\n",
    "    search_results['distances'][0]\n",
    "), 1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(f\"Source: {metadata['source']}\")\n",
    "    print(f\"Similarity Score: {1 - distance:.4f}\")\n",
    "    print(f\"Content: {doc[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2372c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2991f74",
   "metadata": {},
   "source": [
    "### use groq any llm to get final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7af0f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23239bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer to 'What is the experience of mahir?':\n",
      "\n",
      "Mahir Baig has **4+ years of data‑science experience**—including **2+ years focused on Generative AI**.  He works as a **Digital Engineer at Sonata Software Ltd. (since December 2023)**, where he provides advanced technical support and builds Azure‑based LLM, RAG, and Agentic‑AI solutions."
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=\"\")\n",
    "\n",
    "# Prepare the context from search results\n",
    "context = \"\\n\\n\".join([\n",
    "    doc for doc in search_results['documents'][0]\n",
    "])\n",
    "\n",
    "# Create the prompt with context and query\n",
    "prompt = f\"\"\"Based on the following context, answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Get completion from Groq\n",
    "completion = groq_client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream and print the response\n",
    "print(f\"\\nAnswer to '{query}':\\n\")\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143b23a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d38ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: division by zero\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    1/0\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e619ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-12-19_16-40-51'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322072f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11c5bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mahir'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072936f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config.constants import TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81064943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mahir:\n",
    "    def __init__(self):\n",
    "        print(socket.gethostname(), TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148e2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mahir 2025-12-19_17-16-22\n"
     ]
    }
   ],
   "source": [
    "a = mahir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f414e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e56558a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition(a=5, b=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5db12f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "572c0927",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "addition() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m addition(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: addition() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "addition(2,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4920f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.azure_blob_service import AzureBlobManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93d3ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = AzureBlobManager(connection_string='DefaultEndpointsProtocol=https;AccountName=poc123;AccountKey=rOkUqgjfGmAH9ViVfv5zuOV4U1rk3IlDKHxjj2unJ/uCz+dBKM7sAdsBBwk2EPdnkAM3Wi1hSMXs+AStUpE1hQ==;EndpointSuffix=core.windows.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17c583f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.container_exists(\"accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce24590d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.container_exists('erwgtwh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869114a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'WhatsApp Image 2025-12-04 at 9.58.59 AM.jpeg'},\n",
       " ['WhatsApp Image 2025-12-04 at 9.58.59 AM.jpeg'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.list_blob_names_and_files(\"accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1e7369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), [])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.list_blob_names_and_files('rejected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e63492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fdbsnjksks is 'cdsdgdsbg'bjm,smk\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"fdbsnjksks is 'cdsdgdsbg'bjm,smk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df2cb5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is defined\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "a :str = \"dgdg\"\n",
    "\n",
    "if a:\n",
    "    print(\"a is defined\")\n",
    "    print(type(a))\n",
    "else:\n",
    "    print(\"a is not defined\")\n",
    "    print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef22e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sonata_POC'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358f8d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blob'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.components.ingest_files import ingest_files_from_azure_blob\n",
    "import os\n",
    "a = ingest_files_from_azure_blob(\"accepted\", 'blob')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a39559",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_files_from_azure_blob(\"accepted\", os.getcwd(),'a656b7a74dbaf2_Mahir_resume.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bb2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sonata_POC\\\\25939_Srikrupa_Cv-1.docx'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(), '25939_Srikrupa_Cv-1.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a128dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.azure_blob_service import AzureBlobManager\n",
    "\n",
    "m = AzureBlobManager()\n",
    "file_path = r'25939_Srikrupa_Cv-1.docx'\n",
    "m.upload_files(\"accepted\", os.path.join(os.getcwd(), 'blob'),\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80d7c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\sonata_poc\\\\blob'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_extension = 'd:\\\\Sonata_POC\\\\blob' \n",
    "file_extension = file_extension.lower()\n",
    "file_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "736eb0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blob'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dir = os.path.basename(file_extension)\n",
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e56fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WhatsApp Image 2025-12-04 at 9.58.59 AM.jpeg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=os.listdir(file_dir)[-1]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afdf633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it ends with docx\n"
     ]
    }
   ],
   "source": [
    "if a.endswith('.docx'):\n",
    "    print('it ends with docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a37c208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, file_extension = os.path.splitext('d:\\\\Sonata_POC\\\\blob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75ebdfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c2e71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sonata_POC\\\\blob'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1621a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.extractor import FileExtractor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c122da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = FileExtractor(file_path=r'd:\\Sonata_POC\\blob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b8308e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"--- Page 1 --- Summary Mahir Baig GenAI Engineer Bangalore 9066340827 mahirbaig2@gmail.com I am a highly Accomplished Data Science professional with 4+ years of experience in predictive modeling, advanced analytics, and machine learning, including 2+ years specializing in Generative AI. Expert in developing and deploying LLM-based solutions, prompt engineering, fine-tuning, and multimodal GenAI applications. Skilled in end-to-end data workflows, from analysis and feature engineering to model development and evaluation, and adept at leveraging GenAI to automate processes, enhance business efficiency, and deliver innovative AI-driven solutions.\\nK E Y S K I L L S — E X P E R I E N C E Technical Skills.\\nPython Generative AI REST API Azure Cognitive services AI Foundry Machine Learning Git & Git Hub Azure (DP100) Analytical Skills Workflow Automation using AI/GenAI Data Visualization Statistical Analysis and Modeling 12/2023 – Present Digital Engineer, Sonata Software Ltd.\\n• Served as a Technical Support Engineer, providing advanced guidance to the break-fix team and resolving complex issues related to Azure Cognitive Services.\\n• Worked extensively with Azure OpenAI solutions, including LLM integration, Retrieval-Augmented Generation (RAG), and Agentic AI workflows, enabling customers to build scalable and intelligent applications.\\n• Developed and maintained comprehensive workflow documentation to ensure consistency, scalability, and effective knowledge sharing within the team.\\n• Supported the implementation and troubleshooting of AI services such as Speech, Translator, and Document Intelligence, ensuring high availability and optimal performance.\\n• Collaborated with engineering and product teams to deliver feedback on platform improvements, feature enhancements, and customer-driven innovations.\\n08/2023 – 12/2023 Data Analyst, Sungalactic Renewable Energy Private Limited • Designed and implemented the Multilingual Sentiment Analysis project for customer reviews of solar installation while collaborating with a diverse team.\\n• Collected and preprocessed customer review data, including data cleaning, standardization, and handling of missing information.\\n• Configured Azure Translator and developed custom scripts to seamlessly translate customer reviews from various languages into English, while also incorporating Azure Language Sentiment Analysis for efficient data processing.\\n• Developed and maintained documentation for the data and preprocessing workflows, ensuring consistency and knowledge sharing among team members.\\n11/2021 – 07/2023 Data Analyst,180 Azimuth.\\n• Conducted thorough quality assessments of solar panel installations, ensuring compliance with industry standards and regulations.\\n• Collaborated on a machine learning project to analyze solar panel performance data, resulting in actionable insights for system optimization and increased energy yield.\\n• Utilized machine learning algorithms for predictive maintenance of solar panels.\\n• Collaborated with cross-functional teams to implement data- driven solutions, resulting in improvement of overall system efficiency.\\n--- Page 2 --- Projects Project- PredictiveLap - Machine Learning-Based Laptop Price Prediction Tech Stack Utilized- Python, Machine Learning, AWS (EC2).\\nDescription- Choosing the ideal laptop in today's technologically advanced world might be difficult due to the vast array of possibilities available across numerous brands and features.\\nAn innovative data science project called PredictiveLap uses machine learning to help IT administration team make optimal decisions about laptop purchases.\\nImpact and Outcomes: • Accurate Price Prediction: Develop a machine learning model that accurately predicts laptop prices based on specifications.\\n• Data Quality: Collect and preprocess diverse data sources, including web scraping of online retailers using BeautifulSoup, to ensure high-quality input for the model.\\n• Feature Engineering: Create meaningful features from both scraped and existing data to enhance pricing predictions.\\n• Model Selection: Identify the best-performing machine learning algorithm for price prediction.\\n• User-Friendly Interface: Design an intuitive web application for users to input specifications and receive instant price estimates.\\n• EC2 Instance: Deploy the machine learning model on an AWS EC2 instance for scalability and availability.\\nProject- Multilingual Sentiment Analysis of Customer Solar Reviews Tech Stack Utilized- Python, Azure Translator, Azure Language Service, Azure orchestration Project Description: Developed a comprehensive project using Azure Translator and Azure Language Sentiment Analysis to analyze customer reviews for solar design and installation services. The project involves translating reviews from multiple languages to English and then performing sentiment analysis to gauge customer satisfaction.\\nKey Highlights: • Data Collection: Gather a dataset of customer reviews in various languages related to solar design and installation services. These reviews can be collected from feedback sent to the customers and stored in blob storage. Clean and preprocess the collected data, including removing irrelevant information and standardizing formats.\\n• Model Building: Utilize Azure Translator to translate reviews from different languages to English, ensuring consistent language for sentiment analysis.\\nCreated an Azure script to interact with the Translator API.\\nEmployed Azure Language Sentiment Analysis to assess the sentiment of each translated review. This service will provide a sentiment score for each review, indicating whether the feedback is positive, negative, or neutral.\\nStore the translated reviews along with their corresponding sentiment scores in Azure Storage for further analysis and reporting.\\n• Orchestration with Azure Logic Apps: Created an Azure Logic App to orchestrate the entire process. It should trigger the translation and sentiment analysis services and manage the flow of data between them.\\n• Outcome Analysis: Analyzed the sentiment scores and generated reports to summarize customer feedback. This can help identify areas of improvement and overall customer satisfaction.\\n.\\nE D U C A T I O N — R.N.S Institute of Technology Powered by TCPDF (www.tcpdf.org) Powered by TCPDF (www.tcpdf.org)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.extract_pdf(filename='26107_Mahir Baig.pdf',file_dir=r'd:\\Sonata_POC\\blob',text_dir=os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49356cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
